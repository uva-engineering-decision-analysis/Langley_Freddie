---

title: 'Introduction to Bayesian Methods'

 # subtitle: ''

author: "Arthur Small"

institute: "Spring 2020" 

date:   SYS 6014 Decision Analysis

output:

  beamer_presentation:

 #   theme: "metropolis"

    theme: "AnnArbor"

    colortheme: "dolphin"

    fonttheme: "structuresmallcapsserif"

    toc: false

    #toc_depth: 3

    slide_level: 3

    fig_width: 3.5

    fig_height: 3

    fig_caption: true

    

   # html_document:

   #  toc: true

   

---



```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

```



```{r load packages, echo=FALSE, include=FALSE}

library(here)

```





```{r make_beamer_slides, include=FALSE, eval= FALSE}

library(rmarkdown)



# See documentation at: https://rdrr.io/cran/rmarkdown/man/beamer_presentation.html

# and at: https://bookdown.org/yihui/rmarkdown/beamer-presentation.html

# and at: https://rdrr.io/cran/rmarkdown/man/render.html



render("", beamer_presentation(

  toc = FALSE,

  slide_level = NULL,

  number_sections = FALSE,

  incremental = FALSE,

  fig_width = 10,

  fig_height = 7,

  fig_crop = TRUE,

  fig_caption = TRUE,

  dev = "pdf",

  df_print = "default",

  theme = "default",

  colortheme = "default",

  fonttheme = "default",

  highlight = "default",

  template = "default",

  keep_tex = FALSE,

  keep_md = FALSE,

  latex_engine = "pdflatex",

  citation_package = c("none", "natbib", "biblatex"),

  self_contained = TRUE,

  includes = NULL,

  md_extensions = NULL,

  pandoc_args = NULL

)

```





### Big picture: data-driven decision-making



You have a set of possible actions you can take.



You have a prediction model or classification model that enables you to *differentiate* between cases, based on data.



Your decision model then enables you to discriminate --- to make the "right" intervention for each different case, rather than just choosing the same action for all cases.

 

### Predictive models



Predictive models may take many different forms. 



  * They may use many, many different types of data

  * They may be built using different analytic techniques: statistical regression, machine learning,$\ldots$



We embrace this diversity of approaches.



But: we want our predictions to include *uncertainty* information.



So: want outputs in form of *probability distributions* over decision-state variables





### Probabilistic predictions









## Bayesian Methods



### Reminder example: conditional probabilities





```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_ch2_social_mobility.png'))

```



###



Related readings: Hoff Chs. 1, 3



```{r, out.width='40%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_cover.png'))

```









### Bayesian methods: Introduction via simple example







Suppose you want to estimate the fraction of a population that is infected with some disease.



$\theta \in [0,1]$ : true value



Test a random sample of $20$ from the population. 



$Y \in \{0,1,\ldots,20\}$ : # of positive results.



Question: What does realized value of $Y$ tell us about the true value of $\theta$?



### Sampling model



$Y | \theta$ ~ binomial$(20,\theta)$: For $y = 0, 1, \ldots, 20$, (i.i.d.)



$$l(y|\theta) = \Pr(Y=y | \theta) = {20 \choose y} \theta^y (1-\theta)^{(20-y)}$$



where $\binom{n}{k} = \frac{n!}{k!(n-k)!}$



$l(y|\theta)$ called the *likelihood function*.



###



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_fig_1-1.png'))

```



###



Idea: For any $0< \theta < 1$, all values of $Y$ are *possible*, but some are more likely than others. 



The likelihood function tells us how likely is each possible observation, for a given $\theta$.



If, say, $Y = 15$, that provides evidence that $\theta$ is not small.



Core of Bayesian reasoning: work out all the different combinations of $Y, \theta$ that could have generated the observed sample data. 



### Prior information



Suppose we have some background knowledge about the likely values of $\theta$. 



Represent this knowledge by means of a *prior distribution* $\pi(\theta)$ over $[0,1]$.



Obviously, there are many (infinitely many) possible such distributions. 



For convenience, we typically choose to model priors as chosen from a parametrized family of distributions.



### The Beta distribution



$$\theta \sim \text{beta}(a,b)$$



Then



$$E[\theta] = \frac{a}{a+b}$$



###



For our case, let's suppose our prior beliefs correspond to:



$$\theta \sim \text{beta}(2,20)$$



###



$$\theta \sim \text{beta}(2,20)$$ 



implies



```{r, out.width='70%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_ch1_beta_moments.png'))

```





### Bayes Theorem



Let $\pi(\theta | y)$ denote our *posterior distribution* over values of $\theta$.



This means: our *updated* beliefs about the likelihood that $\theta$ takes various values, *after* we've received our test results.



Bayes Theorem says:



$$\pi(\theta | y) = \frac{l(y|\theta) \pi(\theta)}{Pr\{Y = y\}} 

                  = \frac{l(y|\theta) \pi(\theta)}{\int_\Theta l(y|\tilde{\theta})\pi(\tilde{\theta}) d\tilde{\theta}}$$



### 



Can be shown: 



If $\theta \sim \text{beta}(2,20)$ and $Y = 0$, then $\theta | y \sim \text{beta}(2,40)$.



More generally:



If $\theta \sim \text{beta}(a,b)$ and $Y = y$, then $\theta | y \sim \text{beta}(a+y,b+20-y)$.



###



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Exp_beta_posterior.png'))

```





### Sensitivity analysis



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_fig_1-2.png'))

```



## Building a predictive model



###



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_diabetes_model.png'))

```



###



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_fig_1-3.png'))

```



###



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_fig_1-4.png'))

```







### Prediction via the predictive distribution



```{r, out.width='100%', fig.align='center', fig.show = 'hold'}

knitr::include_graphics(here('slides','graphics','Hoff_ch3_predictive_distribution.png'))

```



